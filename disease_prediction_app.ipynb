{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AJ7J4n-1wuP"
      },
      "source": [
        "Our application allows for patients to input some symptoms into a symptom tracker.  This information is then associated to some possible diagnoses.\n",
        "\n",
        "Some new technologies we used (we did not cover in our boot camp) are:\n",
        "1. **SentencePiece** is a supplement to our NLTK.  This supplement is needed to assist in translating medical terms or more complex words.\n",
        "2. **%%capture** is unique to Google Colab.  This allows for the !pip installs to run without generating all the responses, which clutter up the application.\n",
        "3. **sqlite** is a lightweight database management system.  Given that we are dealing with large dataset(s) for our model, sqlite allows our application to store and retreive data using SQL (structured query language.)  We are using this for efficiency and speed of use.\n",
        "4. **Flagging** we added this feature to our gradio interface.  It is used to collect information from users about how the application is working. It is part of improving the model over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4s96ikCWJ2D"
      },
      "outputs": [],
      "source": [
        "# Our pip installs needed to run our application.  Note the %%capture being used is for google colab only.\n",
        "# Remove if you are going to run this in VSCode.\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install gradio\n",
        "!pip intall nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow_text\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install sqlite3\n",
        "!pip install extract_named_entities_nltk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBhc0mcPWqf7"
      },
      "outputs": [],
      "source": [
        "# Imports needed for this application\n",
        "import gradio as gr\n",
        "import torch\n",
        "import sentencepiece\n",
        "import tensorflow\n",
        "import tensorflow_hub\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFfFp7kox6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f4bd1709-e014-4e04-b086-e5dd0909260a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a571a8f-e4c4-4236-955a-2b14e8d88bdf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a571a8f-e4c4-4236-955a-2b14e8d88bdf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving symbipredict.csv to symbipredict (3).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jFTJ6DhqQ-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b94011c-f4ab-448f-a49f-c9c95389c4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                       <<<<<<< HEAD\n",
            "Disease          Symptom_1 Symptom_2            Symptom_3            Symptom_4           Symptom_5 Symptom_6 Symptom_7 Symptom_8 Symptom_9 Symptom_1 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15 Symptom_16   Symptom_17\n",
            "Fungal Infection itching   skin_rash            nodal_skin_eruptions dischromic _patches NaN       NaN       NaN       NaN       NaN       NaN       NaN        NaN        NaN        NaN        NaN        NaN                 NaN\n",
            "                 skin_rash nodal_skin_eruptions dischromic _patches  NaN                 NaN       NaN       NaN       NaN       NaN       NaN       NaN        NaN        NaN        NaN        NaN        NaN                 NaN\n",
            "                 itching   nodal_skin_eruptions dischromic _patches  NaN                 NaN       NaN       NaN       NaN       NaN       NaN       NaN        NaN        NaN        NaN        NaN        NaN                 NaN\n",
            "                           skin_rash            dischromic _patches  NaN                 NaN       NaN       NaN       NaN       NaN       NaN       NaN        NaN        NaN        NaN        NaN        NaN                 NaN\n"
          ]
        }
      ],
      "source": [
        "#  Read the .csv using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data, specifying error handling and potential delimiter\n",
        "disease_data = pd.read_csv('symbipredict.csv', on_bad_lines='skip', delimiter=',') # Added on_bad_lines='skip' and delimiter=','\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(disease_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4GiC3qvsT1K"
      },
      "outputs": [],
      "source": [
        "# After loading the data, it is necessary to combine the symptom_columns into a single column\n",
        "symptom_columns = [col for col in disease_data.columns if col != 'Disease']\n",
        "disease_data['Processed_Symptoms'] = disease_data[symptom_columns].apply(lambda x: ' '.join(x.astype(str)), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4jEvV2GdmEI"
      },
      "source": [
        "In the section below, we import the necessary libraries and dictionaries in order to build our NLTK model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWaKd3Drusfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91fecf8-afad-4d9c-9019-17044b42fbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries starting with nltk\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4yaBUKy4pb2"
      },
      "source": [
        "In the section below, we are defining how we want to use our dataset(s).  We want patients to input their symptoms, then we associate them to key words from our dataset(s).  This is our preprocessing of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Qbiwssm0Ys"
      },
      "outputs": [],
      "source": [
        "# Define the prepocessing of the data\n",
        "def preprocess_symptoms(symptom_text):\n",
        "    tokens = word_tokenize(symptom_text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Example synonym dictionary (this can be expanded)\n",
        "synonym_dict = {\n",
        "    'fever': ['fever', 'pyrexia'],\n",
        "    'headache': ['headache', 'migraine', 'cephalalgia'],\n",
        "    'nausea': ['nausea', 'queasiness', 'sickness'],\n",
        "    'vomiting': ['vomiting', 'throwing up', 'emesis'],\n",
        "    'sore throat': ['sore throat', 'pharyngitis', 'throat pain']\n",
        "}\n",
        "\n",
        "def expand_keywords(keywords):\n",
        "    expanded_keywords = set()\n",
        "    for keyword in keywords:\n",
        "        if keyword in synonym_dict:\n",
        "            expanded_keywords.update(synonym_dict[keyword])\n",
        "        else:\n",
        "            expanded_keywords.add(keyword)\n",
        "    return list(expanded_keywords)\n",
        "\n",
        "def extract_keywords(patient_feedback):\n",
        "    tokens = word_tokenize(patient_feedback.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]\n",
        "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
        "    keywords = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('JJ') or pos.startswith('VB')]\n",
        "    return keywords\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66d7zFr_5FLf"
      },
      "source": [
        "In the section below, the application will read the patient input and suggest diagnosis.  This is our vectorizing process.  Once we vectorize, we run the gradio app, which generates an input cell for patient data, and an output cell for possible diagnoses.  We chose to use Transformers (vectorizing) via TF-IDF becausee it performed better with gradio.  SpaCy and gradio had constant version conflicts, which caused our application to break, so we switched to TF-IDF.\n",
        "\n",
        "NOTE:  We are allowing for a possible 5 diagnoses.  Many symptoms cross over numerous diagnoses.  For now, our app is merely suggesting some possible diagnoses.  Our future model will be more precise.  More data is needed to establish that kind of precision.  Given these challenges and the short runway of time we had to develop this application, we decided to put in a patient feedback loop in our gradio application called flagging.  This allows the patient to tell us if the proposed diagnoses are \"Correct\", \"Incorrect\", \"Needs Improvement\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "814xz1CYvFfm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "47e833eb-7a70-41d6-d3de-0fdf2e6eab0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flagged data should be saved in: /content/custom_flagged_data\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://320b4a996afd182a34.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://320b4a996afd182a34.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Use the preloaded disease_data\n",
        "def extract_keywords(feedback):\n",
        "    # Placeholder for actual keyword extraction logic\n",
        "    return feedback.split()\n",
        "\n",
        "def expand_keywords(keywords):\n",
        "    # Placeholder for actual keyword expansion logic\n",
        "    return keywords\n",
        "\n",
        "def suggest_diagnosis_tfidf(patient_feedback):\n",
        "    keywords = extract_keywords(patient_feedback)\n",
        "    expanded_keywords = expand_keywords(keywords)\n",
        "    print(\"Expanded Keywords:\", expanded_keywords)\n",
        "\n",
        "    processed_feedback = ' '.join(expanded_keywords)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    symptom_matrix = vectorizer.fit_transform(disease_data['Processed_Symptoms'])\n",
        "    feedback_vector = vectorizer.transform([processed_feedback])\n",
        "    similarities = cosine_similarity(feedback_vector, symptom_matrix)\n",
        "    sorted_indices = similarities.argsort()[0][::-1]\n",
        "\n",
        "    possible_diagnoses = []\n",
        "    added_diseases = set()  # To track added diagnoses and avoid duplicates\n",
        "    for idx in sorted_indices:\n",
        "        disease_name = disease_data.loc[idx, 'Disease']\n",
        "        if disease_name not in added_diseases:\n",
        "            possible_diagnoses.append(disease_name)\n",
        "            added_diseases.add(disease_name)\n",
        "\n",
        "    if not possible_diagnoses:\n",
        "        possible_diagnoses = [\"Unable to determine a diagnosis based on the provided information.\"]\n",
        "\n",
        "    return possible_diagnoses[:5]  # Return top 5 possible diagnoses\n",
        "\n",
        "# Ensure the custom flagged directory exists\n",
        "custom_flagged_dir = 'custom_flagged_data'\n",
        "if not os.path.exists(custom_flagged_dir):\n",
        "    os.makedirs(custom_flagged_dir)\n",
        "print(f\"Flagged data should be saved in: {os.path.abspath(custom_flagged_dir)}\")\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=suggest_diagnosis_tfidf,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Describe your symptoms...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Symptom Checker\",\n",
        "    description=\"Enter your symptoms, and we'll suggest possible diagnoses.\",\n",
        "    flagging_mode=\"manual\",\n",
        "    flagging_options=[\"Correct\", \"Incorrect\", \"Needs Improvement\"],\n",
        "    flagging_dir=custom_flagged_dir\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmL9sN5aw5OQ"
      },
      "source": [
        "**For Future Development**\n",
        " Our future development will consist of expanding our application to include treatment recommendations for the diagnoses output.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}