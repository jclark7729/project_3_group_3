{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AJ7J4n-1wuP"
      },
      "source": [
        "Our application allows for patients to input some symptoms into a symptom tracker.  This information is then associated to some possible diagnoses.\n",
        "\n",
        "Some new technologies we used (we did not cover in our boot camp) are:\n",
        "1. **SentencePiece** is a supplement to our NLTK.  This supplement is needed to assist in translating medical terms or more complex words.\n",
        "2. **%%capture** is unique to Google Colab.  This allows for the !pip installs to run without generating all the responses, which clutter up the application.\n",
        "3. **sqlite** is a lightweight database management system.  Given that we are dealing with large dataset(s) for our model, sqlite allows our application to store and retreive data using SQL (structured query language.)  We are using this for efficiency and speed of use.\n",
        "4. **Flagging** we added this feature to our gradio interface.  It is used to collect information from users about how the application is working. It is part of improving the model over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4s96ikCWJ2D"
      },
      "outputs": [],
      "source": [
        "# Our pip installs needed to run our application.  Note the %%capture being used is for google colab only.\n",
        "# Remove if you are going to run this in VSCode.\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install gradio\n",
        "!pip intall nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow_text\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install sqlite3\n",
        "!pip install extract_named_entities_nltk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBhc0mcPWqf7"
      },
      "outputs": [],
      "source": [
        "# Imports needed for this application\n",
        "import gradio as gr\n",
        "import torch\n",
        "import sentencepiece\n",
        "import tensorflow\n",
        "import tensorflow_hub\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFfFp7kox6fb"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jFTJ6DhqQ-l"
      },
      "outputs": [],
      "source": [
        "#  Read the .csv using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "disease_data = pd.read_csv('symbipredict.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(disease_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4GiC3qvsT1K"
      },
      "outputs": [],
      "source": [
        "# After loading the data, it is necessary to combine the symptom_columns into a single column\n",
        "symptom_columns = [col for col in disease_data.columns if col != 'Disease']\n",
        "disease_data['Processed_Symptoms'] = disease_data[symptom_columns].apply(lambda x: ' '.join(x.astype(str)), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4jEvV2GdmEI"
      },
      "source": [
        "In the section below, we import the necessary libraries and dictionaries in order to build our NLTK model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWaKd3Drusfc"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries starting with nltk\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4yaBUKy4pb2"
      },
      "source": [
        "In the section below, we are defining how we want to use our dataset(s).  We want patients to input their symptoms, then we associate them to key words from our dataset(s).  This is our preprocessing of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Qbiwssm0Ys"
      },
      "outputs": [],
      "source": [
        "# Define the prepocessing of the data\n",
        "def preprocess_symptoms(symptom_text):\n",
        "    tokens = word_tokenize(symptom_text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Example synonym dictionary (this can be expanded)\n",
        "synonym_dict = {\n",
        "    'fever': ['fever', 'pyrexia'],\n",
        "    'headache': ['headache', 'migraine', 'cephalalgia'],\n",
        "    'nausea': ['nausea', 'queasiness', 'sickness'],\n",
        "    'vomiting': ['vomiting', 'throwing up', 'emesis'],\n",
        "    'sore throat': ['sore throat', 'pharyngitis', 'throat pain']\n",
        "}\n",
        "\n",
        "def expand_keywords(keywords):\n",
        "    expanded_keywords = set()\n",
        "    for keyword in keywords:\n",
        "        if keyword in synonym_dict:\n",
        "            expanded_keywords.update(synonym_dict[keyword])\n",
        "        else:\n",
        "            expanded_keywords.add(keyword)\n",
        "    return list(expanded_keywords)\n",
        "\n",
        "def extract_keywords(patient_feedback):\n",
        "    tokens = word_tokenize(patient_feedback.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]\n",
        "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
        "    keywords = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('JJ') or pos.startswith('VB')]\n",
        "    return keywords\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66d7zFr_5FLf"
      },
      "source": [
        "In the section below, the application will read the patient input and suggest diagnosis.  This is our vectorizing process.  Once we vectorize, we run the gradio app, which generates an input cell for patient data, and an output cell for possible diagnoses.  We chose to use Transformers (vectorizing) via TF-IDF becausee it performed better with gradio.  SpaCy and gradio had constant version conflicts, which caused our application to break, so we switched to TF-IDF.\n",
        "\n",
        "NOTE:  We are allowing for a possible 5 diagnoses.  Many symptoms cross over numerous diagnoses.  For now, our app is merely suggesting some possible diagnoses.  Our future model will be more precise.  More data is needed to establish that kind of precision.  Given these challenges and the short runway of time we had to develop this application, we decided to put in a patient feedback loop in our gradio application called flagging.  This allows the patient to tell us if the proposed diagnoses are \"Correct\", \"Incorrect\", \"Needs Improvement\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "814xz1CYvFfm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Use the preloaded disease_data\n",
        "def extract_keywords(feedback):\n",
        "    # Placeholder for actual keyword extraction logic\n",
        "    return feedback.split()\n",
        "\n",
        "def expand_keywords(keywords):\n",
        "    # Placeholder for actual keyword expansion logic\n",
        "    return keywords\n",
        "\n",
        "def suggest_diagnosis_tfidf(patient_feedback):\n",
        "    keywords = extract_keywords(patient_feedback)\n",
        "    expanded_keywords = expand_keywords(keywords)\n",
        "    print(\"Expanded Keywords:\", expanded_keywords)\n",
        "\n",
        "    processed_feedback = ' '.join(expanded_keywords)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    symptom_matrix = vectorizer.fit_transform(disease_data['Processed_Symptoms'])\n",
        "    feedback_vector = vectorizer.transform([processed_feedback])\n",
        "    similarities = cosine_similarity(feedback_vector, symptom_matrix)\n",
        "    sorted_indices = similarities.argsort()[0][::-1]\n",
        "\n",
        "    possible_diagnoses = []\n",
        "    added_diseases = set()  # To track added diagnoses and avoid duplicates\n",
        "    for idx in sorted_indices:\n",
        "        disease_name = disease_data.loc[idx, 'Disease']\n",
        "        if disease_name not in added_diseases:\n",
        "            possible_diagnoses.append(disease_name)\n",
        "            added_diseases.add(disease_name)\n",
        "\n",
        "    if not possible_diagnoses:\n",
        "        possible_diagnoses = [\"Unable to determine a diagnosis based on the provided information.\"]\n",
        "\n",
        "    return possible_diagnoses[:5]  # Return top 5 possible diagnoses\n",
        "\n",
        "# Ensure the custom flagged directory exists\n",
        "custom_flagged_dir = 'custom_flagged_data'\n",
        "if not os.path.exists(custom_flagged_dir):\n",
        "    os.makedirs(custom_flagged_dir)\n",
        "print(f\"Flagged data should be saved in: {os.path.abspath(custom_flagged_dir)}\")\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=suggest_diagnosis_tfidf,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Describe your symptoms...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Symptom Checker\",\n",
        "    description=\"Enter your symptoms, and we'll suggest possible diagnoses.\",\n",
        "    flagging_mode=\"manual\",\n",
        "    flagging_options=[\"Correct\", \"Incorrect\", \"Needs Improvement\"],\n",
        "    flagging_dir=custom_flagged_dir\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmL9sN5aw5OQ"
      },
      "source": [
        "**For Future Development**\n",
        " Our future development will consist of expanding our application to include treatment recommendations for the diagnoses output.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}