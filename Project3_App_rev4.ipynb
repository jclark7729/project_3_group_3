{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zqAudEwvtFOo"
      },
      "outputs": [],
      "source": [
        "# New technology used\n",
        "#  %%capture\n",
        "# SentencePiece as a supplement to NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our application allows for patients to input some symptoms into a symptom tracker.  This information is then associated to some possible diagnoses.\n",
        "\n",
        "Some new technologies we used (we did not cover in our boot camp) are:\n",
        "1. **SentencePiece**, which is a supplement to our NLTK.  This supplement is needed to assist in translating medical terms or more complex words.\n",
        "2.**%%capture** which is unique to Google Colab.  This allows for the !pip installs to run without generating all the responses, which clutter up the application.\n",
        "3. **sqlite** which is a lightweight database management system.  Given that we are dealing with large dataset(s) for our model, sqlite allows our application to store and retreive data using SQL (structured query language.)  We are using this for efficiency and speed of use."
      ],
      "metadata": {
        "id": "8AJ7J4n-1wuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "k4s96ikCWJ2D"
      },
      "outputs": [],
      "source": [
        "# Our pip installs needed to run our application.  Note the %%capture being used is for google colab only.\n",
        "# Remove if you are going to run this in VSCode.\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install gradio\n",
        "!pip intall nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow_text\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install sqlite3\n",
        "!pip install extract_named_entities_nltk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "DBhc0mcPWqf7"
      },
      "outputs": [],
      "source": [
        "# Imports needed for this application\n",
        "import gradio as gr\n",
        "import torch\n",
        "import sentencepiece\n",
        "import tensorflow\n",
        "import tensorflow_hub\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "QFfFp7kox6fb",
        "outputId": "29749939-23f1-4643-8886-8edd5707e747"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10de2cb6-7a78-44fd-b978-db2fc8ebabab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10de2cb6-7a78-44fd-b978-db2fc8ebabab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving symbipredict.csv to symbipredict (2).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Read the .csv using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "disease_data = pd.read_csv('symbipredict.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(disease_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jFTJ6DhqQ-l",
        "outputId": "4842e794-2c52-4461-b64a-692cb5adbd2e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Disease  Symptom_1             Symptom_2             Symptom_3  \\\n",
            "0  Fungal Infection    itching             skin_rash  nodal_skin_eruptions   \n",
            "1  Fungal Infection  skin_rash  nodal_skin_eruptions   dischromic _patches   \n",
            "2  Fungal Infection    itching  nodal_skin_eruptions   dischromic _patches   \n",
            "3  Fungal Infection    itching             skin_rash   dischromic _patches   \n",
            "4  Fungal Infection    itching             skin_rash  nodal_skin_eruptions   \n",
            "\n",
            "             Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8 Symptom_9  \\\n",
            "0  dischromic _patches       NaN       NaN       NaN       NaN       NaN   \n",
            "1                  NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "2                  NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "3                  NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "4                  NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "  Symptom_1.1 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n",
            "0         NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "1         NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "2         NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "3         NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "4         NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "\n",
            "  Symptom_16 Symptom_17  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the data, it is necessary to combine the symptom_columns into a single column\n",
        "symptom_columns = [col for col in disease_data.columns if col != 'Disease']\n",
        "disease_data['Processed_Symptoms'] = disease_data[symptom_columns].apply(lambda x: ' '.join(x.astype(str)), axis=1)"
      ],
      "metadata": {
        "id": "p4GiC3qvsT1K"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4jEvV2GdmEI"
      },
      "source": [
        "In the section below, we import the necessary libraries and dictionaries in order to build our NLTK model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries starting with nltk\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWaKd3Drusfc",
        "outputId": "ace4b579-1e42-449d-8405-83b8c8eee590"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the section below, we are defining how we want to use our dataset(s).  We want patients to input their symptoms, then we associate them to key words from our dataset(s).  This is our preprocessing of the model."
      ],
      "metadata": {
        "id": "T4yaBUKy4pb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "s1Qbiwssm0Ys"
      },
      "outputs": [],
      "source": [
        "# Define the prepocessing of the data\n",
        "def preprocess_symptoms(symptom_text):\n",
        "    tokens = word_tokenize(symptom_text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Example synonym dictionary (this can be expanded)\n",
        "synonym_dict = {\n",
        "    'fever': ['fever', 'pyrexia'],\n",
        "    'headache': ['headache', 'migraine', 'cephalalgia'],\n",
        "    'nausea': ['nausea', 'queasiness', 'sickness'],\n",
        "    'vomiting': ['vomiting', 'throwing up', 'emesis'],\n",
        "    'sore throat': ['sore throat', 'pharyngitis', 'throat pain']\n",
        "}\n",
        "\n",
        "def expand_keywords(keywords):\n",
        "    expanded_keywords = set()\n",
        "    for keyword in keywords:\n",
        "        if keyword in synonym_dict:\n",
        "            expanded_keywords.update(synonym_dict[keyword])\n",
        "        else:\n",
        "            expanded_keywords.add(keyword)\n",
        "    return list(expanded_keywords)\n",
        "\n",
        "def extract_keywords(patient_feedback):\n",
        "    tokens = word_tokenize(patient_feedback.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]\n",
        "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
        "    keywords = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('JJ') or pos.startswith('VB')]\n",
        "    return keywords\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the section below, the application will read the patient input and suggest diagnosis.  This is our vectorizing process.  Once we vectorize, we run the gradio app, which generates an input cell for patient data, and an output cell for possible diagnoses.\n",
        "\n",
        "NOTE:  We are allowing for a possible 5 diagnoses.  Many symptoms cross over many diagnoses.  For now, we are merely suggesting some possible diagnoses.  Our future model will be much more precise.  Much more data is needed to establish that kind of precision.  Given these challenges and the short runway of time we had to develop this application, we decided to put in a patient feedback loop in our gradio application.  This allows the patient to tell us if the proposed diagnoses are \"Correct\", \"Incorrect\", \"Needs Improvement\"."
      ],
      "metadata": {
        "id": "66d7zFr_5FLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Use the preloaded disease_data\n",
        "def extract_keywords(feedback):\n",
        "    # Placeholder for actual keyword extraction logic\n",
        "    return feedback.split()\n",
        "\n",
        "def expand_keywords(keywords):\n",
        "    # Placeholder for actual keyword expansion logic\n",
        "    return keywords\n",
        "\n",
        "def suggest_diagnosis_tfidf(patient_feedback):\n",
        "    keywords = extract_keywords(patient_feedback)\n",
        "    expanded_keywords = expand_keywords(keywords)\n",
        "    print(\"Expanded Keywords:\", expanded_keywords)\n",
        "\n",
        "    processed_feedback = ' '.join(expanded_keywords)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    symptom_matrix = vectorizer.fit_transform(disease_data['Processed_Symptoms'])\n",
        "    feedback_vector = vectorizer.transform([processed_feedback])\n",
        "    similarities = cosine_similarity(feedback_vector, symptom_matrix)\n",
        "    sorted_indices = similarities.argsort()[0][::-1]\n",
        "\n",
        "    possible_diagnoses = []\n",
        "    added_diseases = set()  # To track added diagnoses and avoid duplicates\n",
        "    for idx in sorted_indices:\n",
        "        disease_name = disease_data.loc[idx, 'Disease']\n",
        "        if disease_name not in added_diseases:\n",
        "            possible_diagnoses.append(disease_name)\n",
        "            added_diseases.add(disease_name)\n",
        "\n",
        "    if not possible_diagnoses:\n",
        "        possible_diagnoses = [\"Unable to determine a diagnosis based on the provided information.\"]\n",
        "\n",
        "    return possible_diagnoses[:5]  # Return top 5 possible diagnoses\n",
        "\n",
        "# Ensure the custom flagged directory exists\n",
        "custom_flagged_dir = 'custom_flagged_data'\n",
        "if not os.path.exists(custom_flagged_dir):\n",
        "    os.makedirs(custom_flagged_dir)\n",
        "print(f\"Flagged data should be saved in: {os.path.abspath(custom_flagged_dir)}\")\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=suggest_diagnosis_tfidf,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Describe your symptoms...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Symptom Checker\",\n",
        "    description=\"Enter your symptoms, and we'll suggest possible diagnoses.\",\n",
        "    flagging_mode=\"manual\",\n",
        "    flagging_options=[\"Correct\", \"Incorrect\", \"Needs Improvement\"],\n",
        "    flagging_dir=custom_flagged_dir\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "814xz1CYvFfm",
        "outputId": "42f91876-8425-4370-bfb5-620fab806e9f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flagged data should be saved in: /content/custom_flagged_data\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://37af0a923dd5e7610a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://37af0a923dd5e7610a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have established the symptom to diagnosis part of the application, our next piece is to make some suggested remedies for the suggested diagnosis.\n",
        "\n",
        "**NOTE:  Usha below is where your piece of coding comes into play.**"
      ],
      "metadata": {
        "id": "HmL9sN5aw5OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-E4e842Puvp",
        "outputId": "1141942e-2d23-4cbe-b42e-410c307d5bc4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq3H5h9hP2s_",
        "outputId": "d6ebb440-f248-41c7-facf-2ae2428502a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tmp',\n",
              " 'libx32',\n",
              " 'bin',\n",
              " 'lib64',\n",
              " 'sys',\n",
              " 'srv',\n",
              " 'dev',\n",
              " 'opt',\n",
              " 'home',\n",
              " 'mnt',\n",
              " 'run',\n",
              " 'proc',\n",
              " 'root',\n",
              " 'usr',\n",
              " 'lib',\n",
              " 'boot',\n",
              " 'media',\n",
              " 'sbin',\n",
              " 'var',\n",
              " 'lib32',\n",
              " 'etc',\n",
              " 'content',\n",
              " '.dockerenv',\n",
              " 'tools',\n",
              " 'datalab',\n",
              " 'python-apt',\n",
              " 'python-apt.tar.xz',\n",
              " 'NGC-DL-CONTAINER-LICENSE',\n",
              " 'cuda-keyring_1.1-1_all.deb']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flagged_dir = 'flagged'\n",
        "for root, dirs, files in os.walk('/'):\n",
        "    if flagged_dir in dirs:\n",
        "        print(f'Flagged directory found at: {os.path.join(root, flagged_dir)}')\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krFpnkQMQGoe",
        "outputId": "de48ede3-6f3f-4884-decc-88f1a4b7dffa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flagged directory found at: /content/.gradio/flagged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_flagged_dir = 'custom_flagged_data'  # or the name you used\n",
        "if os.path.exists(custom_flagged_dir):\n",
        "    print(f'Custom flagged directory found at: {custom_flagged_dir}')\n",
        "    print(os.listdir(custom_flagged_dir))\n",
        "else:\n",
        "    print(f'Custom flagged directory {custom_flagged_dir} not found.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ophcD5yHQTPj",
        "outputId": "9db7d804-4459-420c-a649-6617f4dc9e47"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom flagged directory found at: custom_flagged_data\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "f_YfyQEPZBIO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "8b5f5dd2-b793-4e23-ac49-5b382c5a9223"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'treatment_database.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ac669956d60e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  Load and Access Treatment Database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtreatment_database\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'treatment_database.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtreatment_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'treatment_database.csv'"
          ]
        }
      ],
      "source": [
        "#  Load and Access Treatment Database\n",
        "treatment_database = pd.read_csv('treatment_database.csv')\n",
        "treatment_database.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMXu577eWOS"
      },
      "outputs": [],
      "source": [
        "# File import\n",
        "import sqlite3\n",
        "        conn = sqlite3.connect('treatment_database.db')\n",
        "        cursor = conn.cursor()\n",
        "        cursor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13sWuWizehtq"
      },
      "outputs": [],
      "source": [
        "# Link the disease to a possible treatment\n",
        "def get_treatments(diagnosed_disease):\n",
        "        treatments = treatment_data[treatment_data['disease'] == diagnosed_disease]['treatment'].tolist()\n",
        "        if treatments:\n",
        "            return treatments\n",
        "        else:\n",
        "            return \"No treatments found for this disease.\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}